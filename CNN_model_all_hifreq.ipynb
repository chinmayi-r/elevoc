{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr=22000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_read(filename, audioFile, metaDataFile, sample_rate=sr):\n",
    "    \"\"\"\n",
    "    Read .wav file, split all vocalizations, and resample to a fixed sample rate for uniformity\n",
    "    Params - \n",
    "    filename: name of wav file\n",
    "    audioPath: path to wav file\n",
    "    metaDataPath: path to txt file containing the start and end times for each vocalization\n",
    "    \n",
    "    Returns - \n",
    "    List containg arrays corresponding to each vocalization in wav file of format:(resampled data of one vocalization in in wav file of size (length of vocalization in seconds*sample rate), length of vocalization in seconds, if the vocalization contains coarse crackles, if the vocalization contains fine crackles)\n",
    "    \"\"\"\n",
    "    print('working on audio: ' + audioFile + ' and metafile: ' + metaDataFile)\n",
    "    recording_annotations = pd.read_csv(metaDataFile,\n",
    "                                        names=['Start', 'End', 'bit1', 'bit2'], delimiter='\\t')\n",
    "\n",
    "    recording_audio = AudioSegment.from_wav(audioFile)\n",
    "    data = [filename]\n",
    "    print(len(recording_annotations.index))\n",
    "\n",
    "    for i in range(len(recording_annotations.index)):\n",
    "        \n",
    "        row = recording_annotations.loc[i]\n",
    "        start = row['Start']\n",
    "        end = row['End']\n",
    "        bit1 = int(row['bit1'])\n",
    "        bit2 = int(row['bit2'])\n",
    "        \n",
    "        audio_chunk = recording_audio[int(start*1000) : int(end*1000)]\n",
    "        \n",
    "        if ((bit1 == 1) & (bit2 == 0)):\n",
    "            if_bit1 = True\n",
    "            if_bit2 = False\n",
    "        elif (bit1 == 0) & (bit2 == 1):\n",
    "            if_bit1 = False\n",
    "            if_bit2 = True\n",
    "        elif (bit1 == 0) & (bit2 == 0):\n",
    "            if_bit1 = False\n",
    "            if_bit2 = False\n",
    "        elif (bit1 == 1) & (bit2 == 1):\n",
    "            if_bit1 = True\n",
    "            if_bit2 = True\n",
    "            \n",
    "        if ((if_bit1 == True) and (if_bit2 == False)):\n",
    "            continue\n",
    "        else:\n",
    "            samples = audio_chunk.get_array_of_samples()\n",
    "            audio_chunk_data = np.array(samples).astype(np.float32)/32768 # convert 24 bit wav file to 16 bit\n",
    "            audio_chunk_resampled = librosa.resample(audio_chunk_data, audio_chunk.frame_rate, sample_rate, res_type='kaiser_best')\n",
    "        \n",
    "            data.append([audio_chunk_resampled, len(audio_chunk), if_bit1, if_bit2])\n",
    "        \n",
    "            print (\"length of audio: \" ,len(audio_chunk))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_voc_list = []\n",
    "voc_lengths = []\n",
    "inputDirectory = '/home/chinmayi/Downloads/Drive/new_input/all'\n",
    "for filename in os.listdir(inputDirectory):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        audioFile = os.path.join(inputDirectory, filename)\n",
    "        metaDataFile = os.path.join(inputDirectory, filename.replace(\".wav\",\".txt\"))\n",
    "        data = split_and_read(filename, audioFile, metaDataFile)\n",
    "        voc_chunk_data = [(d[0], d[2], d[3]) for d in data[1:]]\n",
    "        all_voc_list.extend(voc_chunk_data)\n",
    "        voc_lengths.extend((d[1]) for d in data[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(all_voc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirp=[]\n",
    "roar=[]\n",
    "trumpet=[]\n",
    "for voc in all_voc_list:\n",
    "    if (voc[1] == False) and (voc[2] == False):\n",
    "        chirp.append(voc)\n",
    "    elif (voc[1] == True) and (voc[2] == True):\n",
    "        roar.append(voc)\n",
    "    elif (voc[1] == False) and (voc[2] == True):\n",
    "        trumpet.append(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chirp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(roar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trumpet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trumpet[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "voc_len_arr = np.array(voc_lengths)/1000\n",
    "plt.plot(voc_len_arr, 'o')\n",
    "print('longest vocalization: ',max(voc_len_arr), \"seconds\")\n",
    "print('shortest vococalization:', min(voc_len_arr), \"seconds\")\n",
    "voc_length = 6\n",
    "print('Petcentage of vocalizations less than ', voc_length, ' seconds long:', np.sum(voc_len_arr < voc_length)/len(voc_len_arr)*100, '%')\n",
    "voc_length = 8\n",
    "print('Petcentage of vocalizations less than ', voc_length, ' seconds long:', np.sum(voc_len_arr < voc_length)/len(voc_len_arr)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Noise Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none = []\n",
    "audioFile = '/home/chinmayi/Downloads/Drive/new_input/nn01a_20171022_000000.wav'\n",
    "\n",
    "voc_lengths = []\n",
    "voc_chunk_data = []\n",
    "print('working on audio: ' + audioFile)\n",
    "recording_audio = AudioSegment.from_wav(audioFile)\n",
    "dur = librosa.get_duration(filename=audioFile)\n",
    "no = 45\n",
    "print('no of files: ', no)\n",
    "data = ['none']\n",
    "for i in range(int(no)):\n",
    "    audio_chunk = recording_audio[int(i*8*1000) : int((i+1)*8*1000)]\n",
    "    samples = audio_chunk.get_array_of_samples()\n",
    "    audio_chunk_data = np.array(samples).astype(np.float32)/32768 # convert 24 bit wav file to 16 bit\n",
    "    audio_chunk_resampled = librosa.resample(audio_chunk_data, audio_chunk.frame_rate, 22000, res_type='kaiser_best')\n",
    "    if_bit1 = False\n",
    "    if_bit2 = True\n",
    "    data.append([audio_chunk_resampled, len(audio_chunk), if_bit1, if_bit2])\n",
    "    print (\"length of audio: \" ,len(audio_chunk))\n",
    "\n",
    "voc_chunk_data = [(d[0], d[2], d[3]) for d in data[1:]]\n",
    "none.extend(voc_chunk_data)\n",
    "voc_lengths.extend((d[1]) for d in data[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(none)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into test and train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirp_train, chirp_test = train_test_split(chirp, test_size = train_test_ratio, random_state=10)\n",
    "roar_train, roar_test  = train_test_split(roar, test_size = train_test_ratio, random_state=10)\n",
    "trumpet_train, trumpet_test = train_test_split(trumpet, test_size = train_test_ratio, random_state=10)\n",
    "none_train, none_test  = train_test_split(none, test_size = train_test_ratio, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of files: \", len(all_voc_list)+len(none))\n",
    "print(\"Train files:\")\n",
    "print(\"None: \", len(none_train))\n",
    "print(\"Chirp: \", len(chirp_train))\n",
    "print(\"Roar: \", len(roar_train))\n",
    "print(\"Trumpet: \", len(trumpet_train))\n",
    "print()\n",
    "print(\"Test files:\")\n",
    "print(\"None: \", len(none_test))\n",
    "print(\"Chirp: \", len(chirp_test))\n",
    "print(\"Roar: \", len(roar_test))\n",
    "print(\"Trumpet: \", len(trumpet_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_stretch(sound, rate=np.random.uniform(low=0.5,high=1.5)):\n",
    "    \"\"\"\n",
    "    Randomly squeeze or strech the sound array in the passed data and return the squeezed file\n",
    "    Params - \n",
    "    sound: Array(soundData, bit1, bit2)\n",
    "    rate: constaints for how much to stretch or sqeeze the file. Default: squeeze or strech so that the new file is 0.5 to 1.5 the speed of the original\n",
    "    \n",
    "    Returns - \n",
    "    Array(squeezed file, bit1, bit2)\n",
    "    \"\"\"\n",
    "    stretched_sound = librosa.effects.time_stretch(sound, rate)\n",
    "    return stretched_sound\n",
    "\n",
    "def stretch_breaths_list(data_list, repeat, append_fraction=1):\n",
    "    \"\"\"\n",
    "    Do rand_stretch for a list of arays to be passed to rand_stretch\n",
    "    Params - \n",
    "    data_list: list of arrays to be passed to rand_stretch\n",
    "    repeat: How many time to pass list to rand_stretch. Essetially, the number of times to mulitply the number of sound arrays by 2\n",
    "    append_fraction: fraction of data_list to randomly sample data_list and pass the sampled list to rand_stretch. By default takes the whole data_list\n",
    "    \"\"\"\n",
    "    streched_data_list = []\n",
    "    streched_data_list.extend(data_list)\n",
    "    for i in range(0, repeat):\n",
    "        np.random.shuffle(data_list)\n",
    "        count = int(len(data_list) * (1-append_fraction))\n",
    "        data_list_cut = data_list[count:]\n",
    "        stretched = []\n",
    "        for data in data_list_cut:\n",
    "            sound = data[0]\n",
    "            stretched.append((rand_stretch(sound), data[1], data[2]))\n",
    "        streched_data_list.extend(stretched)\n",
    "    return streched_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(data, final_len_seconds, sample_rate=22000):\n",
    "    \"\"\"\n",
    "    Resize the sound file passed to it to a particular size so that it can be trained. The model requires all the arrays to be trained to be the same size\n",
    "    If the sound file is smaller than the requied lenght to be padded, copy it a whole number of times and pad the rest with zeros\n",
    "    If the sound file is bigger than the required length, squeeze it till it is the right length\n",
    "    Params - \n",
    "    data: Array(soundData, bit1, bit2)\n",
    "    final_len_seconds: length in seconds the file needs to be resized to\n",
    "    sample_rate: current sample rate of the file\n",
    "    \n",
    "    Returns - \n",
    "    Array(ResizedSoundData, bit1, bit2)\n",
    "    \n",
    "    \"\"\"\n",
    "    len_to_fill = int(final_len_seconds * sample_rate)\n",
    "    sound = data[0]\n",
    "    len_filled = len(sound)\n",
    "    len_filled_seconds = len_filled / sample_rate \n",
    "    initial_sound = sound\n",
    "    if (len(initial_sound) < len_to_fill):\n",
    "        for n in range(0, int(final_len_seconds//len_filled_seconds)-1):\n",
    "            initial_sound=np.concatenate((initial_sound,sound), axis=0)\n",
    "        len_to_pad = len_to_fill-len(initial_sound)\n",
    "        final_data = np.pad(initial_sound, [(0, len_to_pad)], mode='constant', constant_values=0)#Pad with zeros\n",
    "    else:\n",
    "        rate=len(initial_sound)/len_to_fill\n",
    "        final_data = librosa.effects.time_stretch(initial_sound, rate)\n",
    "    return (final_data, data[1], data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 25/1000   # in seconds \n",
    "filters_no = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/\n",
    "def hz2mel(hz):\n",
    "    return 1125 * np.log(1 + hz / 700)\n",
    "\n",
    "def mel2hz(mel):\n",
    "    return 700 * (np.exp(mel/1125) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is the formula to calculate mel filterbanks for a particular frequency:\n",
    "##### Reference: http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/\n",
    "<img src=\"../code/formula_blackbg.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mel_filterbanks(audio, sample_rate, filters_no, window_length=0.023233, window_func = 'hamming', fmin=None, fmax=None):\n",
    "    \"\"\"\n",
    "    Create melspectrogram from the audio sample.\n",
    "    References: https://github.com/jameslyons/python_speech_features ; https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html\n",
    "                https://sci-hub.se/10.1109/ICASSP.2013.6639061\n",
    "    Params - sound file, sample rate, number of filters needed, window lenth for sfft, window function for sfft, \n",
    "\n",
    "    Returns -\n",
    "    Filterbanks as cube root values reshaped to constant float array\n",
    "\n",
    "    \"\"\"\n",
    "    windows_no = int(window_length*sample_rate)   # no of windows to use for a 25 ms window length\n",
    "\n",
    "    (f, t, Sxx) = scipy.signal.spectrogram(audio,fs = sample_rate, nfft= windows_no, nperseg=windows_no, window=window_func)    # Get frequency spectrogram signal\n",
    "\n",
    "    if fmin is None:\n",
    "        fmin = min(f)\n",
    "    if fmax is None:\n",
    "        fmax = max(f)\n",
    "\n",
    "    freq_slice = np.where((f >= fmin) & (f <= fmax)) # Remove frequencies above fmax\n",
    "    freq_bins = f[freq_slice]\n",
    "    Sxx = Sxx[freq_slice,:][0]\n",
    "\n",
    "    max_mel = hz2mel(max(freq_bins))\n",
    "    min_mel = hz2mel(min(freq_bins))\n",
    "    mel_bins = np.linspace(min_mel, max_mel, num = (filters_no+2))      # Creating the mel scale f() in the above equation\n",
    "    mel_freq_bins = mel2hz(mel_bins)  \n",
    "\n",
    "    all_fbanks = []\n",
    "    for m in range(1, filters_no+1): \n",
    "        fbank = []\n",
    "        for k in freq_bins:\n",
    "            if(k < mel_freq_bins[m-1]):\n",
    "                hm = 0\n",
    "            elif(k < mel_freq_bins[m]):\n",
    "                hm = (k - mel_freq_bins[m-1]) / (mel_freq_bins[m] - mel_freq_bins[m-1])\n",
    "            elif(k < mel_freq_bins[m + 1]):\n",
    "                hm = (mel_freq_bins[m+1] - k) / (mel_freq_bins[m + 1] - mel_freq_bins[m])\n",
    "            else:\n",
    "                hm = 0\n",
    "            fbank.append(hm)\n",
    "        all_fbanks.append(fbank)\n",
    "    all_fbanks = np.array(all_fbanks).astype(np.float32)\n",
    "\n",
    "    sound_mel_filterbanks = np.matmul(all_fbanks, Sxx)\n",
    "    cbrt_mel_fbanks = np.cbrt(sound_mel_filterbanks  + float(10e-12))   # Cube root is taken here instead of log as it is much more robust\n",
    "                                                        # Adding a very small number to prevent log 0\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(cbrt_mel_fbanks)\n",
    "    scaler.data_max_\n",
    "    norm_cbrt_mel_fbanks=scaler.transform(cbrt_mel_fbanks)    # Normalize sound\n",
    "    return np.reshape(norm_cbrt_mel_fbanks, (filters_no,Sxx.shape[1],1)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(data):\n",
    "    \"\"\"\n",
    "    One hot encode the data for training\n",
    "    Params - \n",
    "    data: flags to one hot encode\n",
    "    \n",
    "   Returns - \n",
    "    one hot encoded label array\n",
    "    \"\"\"\n",
    "    if (data[1] == True) & (data[2] == False):\n",
    "        one_hot_label = np.array([1,0,0,0])\n",
    "    elif (data[1] == False) & (data[2] == False):\n",
    "        one_hot_label = np.array([0,1,0,0])\n",
    "    elif (data[1] == True) & (data[2] == True):\n",
    "        one_hot_label = np.array([0,0,1,0])\n",
    "    elif (data[1] == False) & (data[2] == True):\n",
    "        one_hot_label = np.array([0,0,0,1])\n",
    "    return one_hot_label  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_mel_and_onehot_list(data_list, final_len_seconds, sample_rate=22000):\n",
    "    \"\"\"\n",
    "    Do resize, get_mel_filterbanks and one_hot_encode on a list of the arrays to be passes\n",
    "    Params - \n",
    "    data_list: List of arrays to be passed\n",
    "    final_len_seconds: Seconds to which all files in array have to be resized\n",
    "    sample_rate: Sample rate of each array\n",
    "    \"\"\"\n",
    "    final_list=[]\n",
    "    for data in data_list:\n",
    "        resized = resize(data, final_len_seconds, sample_rate)\n",
    "        mel = get_mel_filterbanks(resized[0], sample_rate, 50)\n",
    "        one_hot_label = one_hot_encode(data)\n",
    "        final_list.append((mel, one_hot_label))\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_stretched_train = stretch_breaths_list(none_train, 2, 0.8) \n",
    "roar_stretched_train = stretch_breaths_list(roar_train,2, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirp_stretched_train = stretch_breaths_list(chirp_train, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trumpet_stretched_train=stretch_breaths_list(trumpet_train, 1, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Train files:\")\n",
    "print(\"None: \", len(none_stretched_train))\n",
    "print(\"Chirp: \", len(chirp_stretched_train))\n",
    "print(\"Roar: \", len(roar_stretched_train))\n",
    "print(\"Trumpet: \", len(trumpet_stretched_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_final_train = resize_mel_and_onehot_list(none_stretched_train,8)\n",
    "chirp_final_train = resize_mel_and_onehot_list(chirp_stretched_train,8)\n",
    "roar_final_train = resize_mel_and_onehot_list(roar_stretched_train,8)\n",
    "trumpet_final_train = resize_mel_and_onehot_list(trumpet_stretched_train,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {'none':none_final_train,'chirp':chirp_final_train,'roar':roar_final_train, 'trumpet':trumpet_final_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_stretched_test = stretch_breaths_list(none_test, 2, 0.8) \n",
    "roar_stretched_test = stretch_breaths_list(roar_test,2, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirp_stretched_test = stretch_breaths_list(chirp_test, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trumpet_stretched_test=stretch_breaths_list(trumpet_test, 1, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test files:\")\n",
    "print(\"None: \", len(none_stretched_test))\n",
    "print(\"Chirp: \", len(chirp_stretched_test))\n",
    "print(\"Roar: \", len(roar_stretched_test))\n",
    "print(\"Trumpet: \", len(trumpet_stretched_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_final_test = resize_mel_and_onehot_list(none_stretched_test,8)\n",
    "chirp_final_test = resize_mel_and_onehot_list(chirp_stretched_test,8)\n",
    "roar_final_test = resize_mel_and_onehot_list(roar_stretched_test,8)\n",
    "trumpet_final_test = resize_mel_and_onehot_list(trumpet_stretched_test,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {'none':none_final_test,'chirp':chirp_final_test,'roar':roar_final_test, 'trumpet':trumpet_final_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_height = train_dict['none'][0][0].shape[0]\n",
    "sample_width = train_dict['none'][0][0].shape[1]\n",
    "ind = 5\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.subplot(4,1,1)\n",
    "plt.imshow(train_dict['none'][ind][0].reshape(sample_height, sample_width))\n",
    "plt.title('None')\n",
    "plt.subplot(4,1,2)\n",
    "plt.imshow(train_dict['chirp'][ind][0].reshape(sample_height, sample_width))\n",
    "plt.title('Chirp')\n",
    "plt.subplot(4,1,3)\n",
    "plt.imshow(train_dict['roar'][ind][0].reshape(sample_height, sample_width))\n",
    "plt.title('Roar')\n",
    "plt.subplot(4,1,4)\n",
    "plt.imshow(train_dict['trumpet'][ind][0].reshape(sample_height, sample_width))\n",
    "plt.title('Trumpet')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "\"\"\"\n",
    "Data generator for both traning and validation. Wraps each sound array on its time axis using FFTroll before feeding to the model\n",
    "Reference: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "\"\"\"\n",
    "class data_generator():\n",
    "    def __init__(self, sound_clips, strides):\n",
    "        self.clips = sound_clips\n",
    "        self.strides = strides\n",
    "        self.lengths = [len(arr) for arr in sound_clips]\n",
    "    \n",
    "    def n_available_samples(self):\n",
    "        return int(min(np.divide(self.lengths, self.strides))) * 4\n",
    "    \n",
    "    def generate_keras(self, batch_size):\n",
    "        cursor = [0,0,0,0]\n",
    "        while True:\n",
    "            i = 0\n",
    "            X,y = [],[]\n",
    "            for c in range(batch_size):\n",
    "                cat_length = self.lengths[i]\n",
    "                cat_clips = self.clips[i]\n",
    "                cat_stride = self.strides[i]\n",
    "                cat_advance = np.random.randint(low= 1,high = cat_stride + 1)\n",
    "                clip = cat_clips[(cursor[i] + cat_advance) % cat_length]\n",
    "                cursor[i] = (cursor[i] + self.strides[i]) % cat_length #advance cursor\n",
    "                s = (self.rollFFT(clip))\n",
    "                X.append(s[0])\n",
    "                y.append(s[1])\n",
    "                i = (i + 1) % 4 # go to next class\n",
    "            yield (np.reshape(X, (batch_size, sample_height, sample_width, 1)),\n",
    "                   np.reshape(y,(batch_size,4)))\n",
    "\n",
    "    #Transpose and wrap each array along the time axis\n",
    "    def rollFFT(self, fft_info):\n",
    "        fft = fft_info[0]\n",
    "        n_col = fft.shape[1]\n",
    "        pivot = np.random.randint(n_col)\n",
    "        return ((np.roll(fft, pivot, axis = 1)), fft_info[1])\n",
    "\n",
    "#Used for validation set\n",
    "class feed_all():\n",
    "    #sound_clips = [[none],[crackles],[wheezes],[both]]\n",
    "    #strides: How far the sampling index for each category is advanced for each step\n",
    "    def __init__(self, sound_clips, roll = True):\n",
    "        merged = []\n",
    "        for arr in sound_clips:\n",
    "            merged.extend(arr)\n",
    "        np.random.shuffle(merged)\n",
    "        self.clips = merged\n",
    "        self.nclips = len(merged)\n",
    "        self.roll = roll\n",
    "    \n",
    "    def n_available_samples(self):\n",
    "        return len(self.clips)\n",
    "    \n",
    "    def generate_keras(self, batch_size):\n",
    "        i = 0\n",
    "        while True:\n",
    "            X,y = [],[]\n",
    "            for b in range(batch_size):\n",
    "                clip = self.clips[i]\n",
    "                i = (i + 1) % self.nclips\n",
    "                if(self.roll):\n",
    "                    s = (self.rollFFT(clip))\n",
    "                    X.append(s[0])\n",
    "                    y.append(s[1])\n",
    "                else:\n",
    "                    X.append(clip[0])\n",
    "                    y.append(clip[1])\n",
    "                    \n",
    "            yield (np.reshape(X, (batch_size,sample_height, sample_width,1)),\n",
    "                   np.reshape(y,(batch_size, 4)))\n",
    "\n",
    "    #Transpose and wrap each array along the time axis\n",
    "    def rollFFT(self, fft_info):\n",
    "        fft = fft_info[0]\n",
    "        n_col = fft.shape[1]\n",
    "        pivot = np.random.randint(n_col)\n",
    "        return ((np.roll(fft, pivot, axis = 1)), fft_info[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data pipeline objects\n",
    "train_gen = data_generator([train_dict['none'], train_dict['chirp'], train_dict['roar'], train_dict['trumpet']], [1,1,1,1])\n",
    "test_gen = feed_all([test_dict['none'], test_dict['chirp'], test_dict['roar'], test_dict['trumpet']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras implementation\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Activation, Dropout, MaxPool2D, Flatten, LeakyReLU\n",
    "import tensorflow as tf\n",
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(128, [7,11], strides = [2,2], padding = 'SAME', input_shape = (sample_height, sample_width, 1)))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(MaxPool2D(padding = 'SAME'))\n",
    "\n",
    "model.add(Conv2D(256, [5,5], padding = 'SAME'))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(MaxPool2D(padding = 'SAME'))\n",
    "\n",
    "model.add(Conv2D(256, [1,1], padding = 'SAME'))\n",
    "model.add(Conv2D(256, [3,3], padding = 'SAME'))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(MaxPool2D(padding = 'SAME'))\n",
    "\n",
    "model.add(Conv2D(512, [1,1], padding = 'SAME'))\n",
    "model.add(Conv2D(512, [3,3], padding = 'SAME',activation = 'relu'))\n",
    "model.add(Conv2D(512, [1,1], padding = 'SAME'))\n",
    "model.add(Conv2D(512, [3,3], padding = 'SAME', activation = 'relu'))\n",
    "model.add(MaxPool2D(padding = 'SAME'))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(4096, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(Dense(4, activation = 'softmax'))\n",
    "\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate = initial_learning_rate,\n",
    "    decay_steps=5,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "opt = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.00, amsgrad=False)\n",
    "\n",
    "#opt = optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=None, amsgrad=False)\n",
    "\n",
    "model.compile(optimizer =  opt , loss = 'categorical_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = model.fit_generator(generator = train_gen.generate_keras(batch_size), \n",
    "                            steps_per_epoch = train_gen.n_available_samples() // batch_size,\n",
    "                            validation_data = test_gen.generate_keras(batch_size),\n",
    "                            validation_steps = test_gen.n_available_samples() // batch_size, \n",
    "                            epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(stats.history['acc'], label = 'training acc')\n",
    "plt.plot(stats.history['val_acc'], label = 'validation acc')\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(stats.history['loss'], label = 'training loss')\n",
    "plt.plot(stats.history['val_loss'], label = 'validation loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_gen.generate_keras(test_gen.n_available_samples()).__next__()\n",
    "predictions = model.predict(test_set[0])\n",
    "predictions = np.argmax(predictions, axis = 1)\n",
    "labels = np.argmax(test_set[1], axis = 1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0, 3, 3, 1, 3, 1, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3,\n",
    "       2, 3, 3, 3, 1, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3,\n",
    "       3, 3, 3, 2, 3, 3, 1, 3, 1, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 1, 3,\n",
    "       2, 2, 1, 3, 3, 3, 2, 2, 1, 2, 1, 2, 3, 3, 3, 3, 1, 3, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(labels, predictions, target_names = ['none','chirp','roar','trumpet']))\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['none','chirp','roar','trumpet']\n",
    "cm_df = pd.DataFrame(cm, columns=labels, index=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.heatmap(cm_df, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./AH1_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd7b75dad365ac5854c3069db75a353297507929f43cb758f259abcc8c65be77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
